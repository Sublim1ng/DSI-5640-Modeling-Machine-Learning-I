---
title: "Homework 5"
author: Ningyu Han
output: github_document
date: "2023-04-03"
---


# Gradient Boosting
### Load required libraries
```{r}
library(gbm)
library(caret)
```

### Load the datasets
```{r}
url_train <- "https://hastie.su.domains/ElemStatLearn/datasets/vowel.train"
vowel_data_train <- read.csv(url_train, header = TRUE, sep = ",")

url_test <- "https://hastie.su.domains/ElemStatLearn/datasets/vowel.test"
vowel_data_test <- read.csv(url_test, header = TRUE, sep = ",")
```

### Split the data
```{r}
train_X <- vowel_data_train[, 3:12]
train_y <- vowel_data_train$y
test_X <- vowel_data_test[, 3:12]
test_y <- vowel_data_test$y

```

### Use 5-fold cross-validation to tune the model
```{r}
tuneGrid <- expand.grid(n.trees = c(100, 200, 300),
                        interaction.depth = c(1, 2, 3),
                        shrinkage = c(0.001, 0.01, 0.1),
                        n.minobsinnode = c(5, 10, 20))

fitControl <- trainControl(method = "cv", number = 5)
model <- train(train_X, train_y, method = "gbm", trControl = fitControl,
               verbose = FALSE, tuneGrid = tuneGrid)
```

### Print the best tuning parameters
```{r}
print(model$bestTune)
```

### Fit the final model with the best tuning parameters
```{r}
final_model <- gbm(y ~ ., data = vowel_data_train[, 2:12], distribution = "multinomial",
                   n.trees = model$bestTune$n.trees,
                   interaction.depth = model$bestTune$interaction.depth,
                   shrinkage = model$bestTune$shrinkage,
                   n.minobsinnode = model$bestTune$n.minobsinnode,
                   cv.folds = 5, verbose = FALSE)
```

### Make predictions
```{r}
test_y_pred <- predict(final_model, newdata = test_X, n.trees = model$bestTune$n.trees, type = "response")
```

### Convert predictions to class labels
```{r}
test_y_pred_labels <- matrix(test_y_pred, ncol = length(unique(train_y)), byrow = TRUE)
test_y_pred_class <- max.col(test_y_pred_labels)
```

### Compute the misclassification rate using the 'vowel.test' data
```{r}
misclassification_rate <- sum(test_y_pred_class != test_y) / length(test_y)
print(paste0("Misclassification rate: ", misclassification_rate))
```


# XGBoosting
```{r}
library(caret)
library(xgboost)
```

### Load Data
```{r}
url <- "https://hastie.su.domains/ElemStatLearn/datasets/vowel.train"
vowel_data_train<- read.csv(url, header = TRUE, sep = ",")
```

```{r}
url <- "https://hastie.su.domains/ElemStatLearn/datasets/vowel.test"
vowel_data_test<- read.csv(url, header = TRUE, sep = ",")
```

### Split Data
```{r}
train_X <- vowel_data_train[, 3:12]
train_y <- vowel_data_train$y
```

### Fit the model
```{r}
mat_x = as.matrix(train_X)
model <- xgboost(data = mat_x, label = train_y, nrounds = 50)
print(model)
```

### Define the tuning parameter grid
```{r}
tuning_grid <- expand.grid(nrounds = c(50, 100, 150),
                           max_depth = c(3, 5, 7),
                           eta = c(0.01, 0.1, 0.3),
                           gamma = c(0, 0.1, 0.3),
                           subsample = c(0.5, 0.7, 1),
                           colsample_bytree = c(0.5, 0.7, 1),
                           min_child_weight = c(1, 3, 5))
```

### Fit the model using 5-fold cross-validation
```{r}
model_tune <- train(mat_x, train_y, method = "xgbTree", 
                    tuneGrid = tuning_grid, 
                    trControl = trainControl(method = "cv", number = 5))
print(model_tune)
```

### Get the best fit
```{r}
print(model_tune$bestTune)
```

### test data
```{r}
test_X <- vowel_data_test[, 3:12]
mat_test_x <- as.matrix(test_X)
test_y <- vowel_data_test$y
```

# With the tuned model, make predictions using the majority vote method
```{r}
model_final <- xgboost(data = mat_x, label = train_y, 
                       nrounds = model_tune$bestTune$nrounds,
                       max_depth = model_tune$bestTune$max_depth,
                       eta = model_tune$bestTune$eta,
                       gamma = model_tune$bestTune$gamma,
                       subsample = model_tune$bestTune$subsample,
                       colsample_bytree = model_tune$bestTune$colsample_bytree,
                       min_child_weight = model_tune$bestTune$min_child_weight)

test_y_pred <- predict(model_final, newdata = as.matrix(test_X))
test_y_pred_num <- as.matrix(test_y_pred)
test_y_pred_majority <- ifelse(rowSums(test_y_pred_num == "0") >= 3, "0", "1")
```

### Compute the misclassification rate using the ‘vowel.test’ data
```{r}
misclassification_rate <- sum(test_y_pred_majority != test_y) / length(test_y)
print(paste0("Misclassification rate: ", misclassification_rate))
```







